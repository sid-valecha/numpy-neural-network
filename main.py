"""
Neural Network from Scratch (NumPy‑only)
=======================================
A fully‑connected feed‑forward classifier for the MNIST handwritten‑digit dataset.
*   No deep‑learning frameworks (PyTorch / TensorFlow / JAX).
*   Pure NumPy for all tensor math, forward‑prop, and back‑prop.
*   Mini‑batch SGD with optional momentum.

Run:
    python neural_network_from_scratch.py --epochs 10 --batch_size 128 --lr 0.01

Tip: Use the ``--sample`` flag to train on a subset while experimenting.
(heading generated by ChatGPT 4o)
"""
from __future__ import annotations

import argparse
import gzip
import os
import urllib.request
from pathlib import Path

import numpy as np

# data loading
_MNIST_URLS = {
    "train_images": "http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz",
    "train_labels": "http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz",
    "test_images": "http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz",
    "test_labels": "http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz",
}


def _download_mnist(dest: Path) -> None:
    dest.mkdir(parents=True, exist_ok=True)
    for name, url in _MNIST_URLS.items():
        out_path = dest / f"{name}.gz"
        if not out_path.exists():
            print(f"→ Downloading {name}…")
            urllib.request.urlretrieve(url, out_path)


def _load_idx(filename: Path) -> np.ndarray:
    """Load an IDX file into a NumPy array."""
    with gzip.open(filename, "rb") as f:
        data = f.read()
    if filename.name.endswith("-images-idx3-ubyte.gz"):
        magic, num, rows, cols = np.frombuffer(data[:16], dtype=">u4")
        return (np.frombuffer(data[16:], dtype=np.uint8).reshape(num, rows * cols) / 255.0).astype("float32")
    else:  # labels
        magic, num = np.frombuffer(data[:8], dtype=">u4")
        return np.frombuffer(data[8:], dtype=np.uint8)


def load_mnist(data_dir: str = "./mnist", sample: int | None = None):
    """Return (x_train, y_train, x_test, y_test)."""
    data_path = Path(data_dir)
    _download_mnist(data_path)

    x_train = _load_idx(data_path / "train_images.gz")
    y_train = _load_idx(data_path / "train_labels.gz")
    x_test = _load_idx(data_path / "test_images.gz")
    y_test = _load_idx(data_path / "test_labels.gz")

    if sample:
        idx = np.random.choice(len(x_train), sample, replace=False)
        x_train, y_train = x_train[idx], y_train[idx]

    return x_train, y_train, x_test, y_test


def one_hot(y: np.ndarray, num_classes: int = 10) -> np.ndarray:
    return np.eye(num_classes)[y]


#neural net components

#activation functions etc
def relu(z: np.ndarray) -> np.ndarray:
    return np.maximum(0.0, z)


def relu_grad(z: np.ndarray) -> np.ndarray:
    return (z > 0).astype(z.dtype)


def softmax(z: np.ndarray) -> np.ndarray:
    z_shift = z - z.max(axis=1, keepdims=True)  # stability
    exp = np.exp(z_shift)
    return exp / exp.sum(axis=1, keepdims=True)


#loss+gradient
def cross_entropy(pred: np.ndarray, target: np.ndarray) -> float:
    eps = 1e-12
    return -np.mean(np.sum(target * np.log(pred + eps), axis=1))


def cross_entropy_grad(pred: np.ndarray, target: np.ndarray) -> np.ndarray:
    return (pred - target) / pred.shape[0]


#weight init helpers

def he_init(fan_in: int, fan_out: int) -> np.ndarray:
    scale = np.sqrt(2.0 / fan_in)
    return np.random.randn(fan_in, fan_out).astype("float32") * scale


class DenseLayer:
    def __init__(self, in_dim: int, out_dim: int):
        self.W = he_init(in_dim, out_dim)
        self.b = np.zeros((1, out_dim), dtype="float32")
        # for momentum
        self.v_W = np.zeros_like(self.W)
        self.v_b = np.zeros_like(self.b)

    def forward(self, x: np.ndarray) -> np.ndarray:
        self.input = x  # cache
        return x @ self.W + self.b

    def backward(self, grad_out: np.ndarray, lr: float, momentum: float):
        grad_W = self.input.T @ grad_out
        grad_b = grad_out.sum(axis=0, keepdims=True)
        grad_input = grad_out @ self.W.T

        #momentum SGD update
        self.v_W = momentum * self.v_W - lr * grad_W
        self.v_b = momentum * self.v_b - lr * grad_b
        self.W += self.v_W
        self.b += self.v_b
        return grad_input


class NeuralNetwork:
    def __init__(self, layer_sizes: list[int], lr: float = 0.01, momentum: float = 0.9):
        self.lr = lr
        self.momentum = momentum
        self.layers: list[DenseLayer] = [
            DenseLayer(layer_sizes[i], layer_sizes[i + 1]) for i in range(len(layer_sizes) - 1)
        ]

    def forward(self, x: np.ndarray) -> np.ndarray:
        for i, layer in enumerate(self.layers):
            x = layer.forward(x)
            if i < len(self.layers) - 1:
                x = relu(x)
        return softmax(x)

    def train_batch(self, x: np.ndarray, y: np.ndarray):
        #forward
        logits = []
        activations = [x]
        for i, layer in enumerate(self.layers):
            z = layer.forward(activations[-1])
            logits.append(z)
            if i < len(self.layers) - 1:
                a = relu(z)
            else:
                a = softmax(z)
            activations.append(a)

        #backward
        grad = cross_entropy_grad(activations[-1], y)
        for i in reversed(range(len(self.layers))):
            grad = self.layers[i].backward(grad, self.lr, self.momentum)
            if i > 0:
                grad *= relu_grad(logits[i - 1])

        return cross_entropy(activations[-1], y)

    def predict(self, x: np.ndarray) -> np.ndarray:
        return np.argmax(self.forward(x), axis=1)

    def accuracy(self, x: np.ndarray, y: np.ndarray) -> float:
        return np.mean(self.predict(x) == y)


#training
def main():
    parser = argparse.ArgumentParser(description="NumPy MNIST MLP")
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=128)
    parser.add_argument("--lr", type=float, default=0.01)
    parser.add_argument("--momentum", type=float, default=0.9)
    parser.add_argument("--hidden", type=int, nargs="*", default=[128, 64])
    parser.add_argument("--sample", type=int, help="Subsample training set (e.g. 10000)")
    args = parser.parse_args()

    x_train, y_train, x_test, y_test = load_mnist(sample=args.sample)
    y_train_oh = one_hot(y_train)

    layer_sizes = [784] + args.hidden + [10]
    net = NeuralNetwork(layer_sizes, lr=args.lr, momentum=args.momentum)

    n_batches = len(x_train) // args.batch_size
    for epoch in range(1, args.epochs + 1):
        perm = np.random.permutation(len(x_train))
        x_train, y_train_oh = x_train[perm], y_train_oh[perm]

        epoch_loss = 0.0
        for i in range(n_batches):
            start = i * args.batch_size
            end = start + args.batch_size
            batch_x = x_train[start:end]
            batch_y = y_train_oh[start:end]
            loss = net.train_batch(batch_x, batch_y)
            epoch_loss += loss

        acc = net.accuracy(x_test, y_test)
        print(f"Epoch {epoch:02d} | Loss: {epoch_loss / n_batches:.4f} | Test Acc: {acc * 100:.2f}%")


if __name__ == "__main__":
    main()
